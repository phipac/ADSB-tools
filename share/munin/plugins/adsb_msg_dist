#!/usr/bin/python
# encoding: utf-8

# ADS-B message distribution analysis plugin for Munin
# Copyright (c) 2017 David King
# https://github.com/strix-technica/ADSB-tools
#
# NB: Requires geopy and (to be of any use) python-daemon.  See README.md
#     for installation and usage instructions.
#
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
# Update 2024/06/11 by Phipac - vincenty no longer supported
# Replaced with geodesic

import collections
import csv
import datetime
from geopy.distance import geodesic
import logging
import logging.handlers
import math
import Queue
import os
import socket
import sys
import threading
import time

# --- Configuration begins ---
SERVER = '192.168.0.6'
PORT   = 30003
TIMER  = 300

LOG_FILE = '/var/log/adsb-msg-dist.log'
STATS_FILE = '/var/run/adsb-msg-dist.dat'
PID_FILE = '/var/run/adsb-msg-dist.pid'

USER   = 'munin' # for daemon mode
GROUP  = 'adm'

# Messages older than this will be ignored
AC_TO  = 600
# --- Configuration ends ---

# Non-config constants
HDR = ['type', 'subtype', 'sid', 'aid', 'icao', 'fid', 'g_date', 'g_time', 'l_date', 'l_time', 'cs', 'alt', 'gs', 'trk', 'lat', 'lon', 'vr', 'squawk', 'sq_flag', 'emerg', 'ident', 'gnd']

KM_PER_FT = 0.0003048 # km per foot


class Message(object):
    """
    ADS-B message class (subset)
    """
    def __init__( self, d ):
        self.ts = datetime.datetime.strptime( d['g_date'] + d['g_time'] + '000',
                                              '%Y/%m/%d%H:%M:%S.%f' )
        self.icao = d['icao']
        self.lat = self.lon = self.alt = self.gs = None

        if d['lat']:
            self.lat = float( d['lat'] )
        if d['lon']:
            self.lon = float( d['lon'] )
        if d['alt']:
            self.alt = float( d['alt'] ) * KM_PER_FT # geopy wants metric
        if d['gs']:
            self.gs  = int( d['gs'] )

    @property
    def pos( self ):
        if self.lat is not None and self.lon is not None and self.alt is not None:
            return (self.lat, self.lon, self.alt,)
        else:
            return None

    def __unicode__( self ):
        return u"%s: %s → %s° %s° %s' @ %s kts" % (
                self.ts.strftime( '%H:%M:%S.%f' ),
                self.icao,
                self.lat and self.lat or '-', self.lon and self.lon or '-',
                self.alt and (self.alt / KM_PER_FT) or '-',
                self.gs and self.gs or '-',)
    def __repr__( self ):
        return unicode( self ).encode( 'utf-8' )


class StreamToLogger(object):
    """
    Fake file-like stream object that redirects writes to a logger instance.
    """
    def __init__(self, logger, handler, log_level=logging.INFO):
        self.logger = logger
        self.handler = handler
        self.log_level = log_level
        self.linebuf = ''

    def write(self, buf):
        for line in buf.rstrip().splitlines():
            self.logger.log(self.log_level, line.rstrip())

    def fileno( self ):
        return self.handler.stream.fileno()


def init_socket():
    """
    (Re)acquire TCP connection to SBS data source.  Returns a CSV reader.
    """
    s = socket.socket( socket.AF_INET, socket.SOCK_STREAM )
    while run:
        try:
            s.connect( (SERVER, PORT,) )
            break
        except socket.error, e:
            logger.info( 'waiting for %s:%s' % (SERVER, PORT,) )
            time.sleep( 0.5 )
    logger.info( '(re)connected to %s:%s' % (SERVER, PORT,) )
    return csv.reader( s.makefile() )


def mean_sd( data ):
    """
    Returns a tuple of (mean, sample_std_deviation) for the given data
    """
    if not data:
        return (None, None,)

    n = len( data )
    mean = sum( data ) / float(n)
    ssd = math.sqrt( sum( (x - mean)**2 for x in data ) / n )

    return (mean, ssd,)


run = True
msgs = Queue.Queue()
def input_thread_entrypoint():
    """
    (TCP) input thread entry point.

    Relays messages via queue `msgs`.
    """
    while True:
        # Attempt to retry connection should the socket close
        s = init_socket()
        for d in s:
            if not run:
                return

            d = dict( zip( HDR, d ) )
            if d.get( 'type' ) == 'MSG' and d.get( 'subtype' ) in ['3', '4']:
                msgs.put( Message( d ) )


def do_ts( last, delta, msg ):
    """
    Calculate and record the time interval since the last message for this a/c
    """
    d = msg.ts - last[msg.icao]['ts']
    d = d.seconds + d.microseconds / 1e6
    delta['ts'].append( d )


def do_pos( last, delta, msg ):
    """
    Calculate and record the distance interval since the last positional message
    for this a/c.  NB: we don't calculate differences on velocity updates.

    Return value is in feet adjusted for last-known ground speed in feet/sec.
    """
    d = geodesic( last[msg.icao]['pos'], msg.pos ).nm
    nm_per_sec = last[msg.icao]['gs'] / 3600.0
    delta['pos'].append( d / nm_per_sec )


def mainline_entrypoint():
    """
    Mainline processing code entry point:
        * process queued messages, combine alt with gs info where possible
        * estimate intervals
        * generate distribution of the resultant intervals
        * write out results for Munin every TIMER seconds and
    """
    # XXX Override for debugging
    #TIMER = 3

    last = collections.defaultdict( lambda: {'ts': None, 'pos': None, 'gs': None} )

    global AC_TO
    AC_TO = datetime.timedelta( seconds = AC_TO )
    while True:
        delta = {'ts': [], 'pos': []}
        time.sleep( TIMER - time.time() % TIMER )
        n = msgs.qsize()

        for i in xrange(n):
            msg = msgs.get()

            # Calculate deltas when:
            #   - we have last position and ground speed, and
            #   - the last information we have is new enough to be considered
            #     part of a contiguous track
            #   - this is a new positional message
            if ( last[msg.icao]['pos'] and
                 last[msg.icao]['gs']  and
                (last[msg.icao]['ts'] > (datetime.datetime.now( ) - AC_TO)) and
                    msg.pos):

                do_ts( last, delta, msg )
                # Process position, not velocity messages
                do_pos( last, delta, msg )

            # Update last-known information; only consider a/c transmitting
            # position and velocity data for timestamps because we need to
            # know their range in order to know how to weight their
            # contribution to the distribution.
            if msg.pos:
                last[msg.icao]['pos'] = msg.pos
                last[msg.icao]['ts']  = msg.ts
            if msg.gs:
                last[msg.icao]['gs'] = msg.gs

        ts_mean, ts_sd = mean_sd( delta['ts'] )
        pos_mean, pos_sd = mean_sd( delta['pos'] )

        with open( STATS_FILE, 'w' ) as f:
            print >>f, """\
ts_sd.value %s
ts_mean.value %s
ts_n.value %s
pos_sd.value %s
pos_mean.value %s
pos_n.value %s
""" % (ts_sd, ts_mean, len(delta['ts']), pos_sd, pos_mean, len(delta['pos']),),

        f.close()


def munin_config():
    """
    Output Munin config data
    """
    print """\
graph_title ADS-B message distribution
graph_vlabel sec, 1 sec displacement ratio
graph_category dump1090
graph_info This graph characterises the statistical distribution of received messages by time (in seconds) and by displacement (as a ratio to displacement in one second at current ground speed) on an a/c by a/c basis.  The mean is given for reference, where the standard deviation is an indication of the quality of reception (on the assumption that each a/c transmits on a fairly periodic basis).  A small s.d. indicates that you're receiving most or nearly all messages from each a/c within range.  If you're losing messages, the difference between messages received will be larger by a factor equal to the number of lost messages (+1) and thus will produce a larger s.d.
ts_sd.label S.d. time interval
ts_sd.info Standard deviation of inter-message interval in seconds
ts_mean.label Mean time interval
ts_mean.info Mean of inter-message interval in seconds
ts_n.label ts sample size
pos_sd.label S.d. normalised displacement ratio
pos_sd.info Standard deviation of inter-message displacement as a ratio to the displacement of the a/c in one second at its last known ground speed
pos_mean.label Mean normalised displacement ratio
pos_mean.info Mean of inter-message displacement as a ratio to the displacement of the a/c in one second at its last known ground speed
pos_n.label pos sample size
""",
    exit()


def munin_data():
    """
    Output recorded Munin data
    """
    if not os.path.isfile( STATS_FILE ):
        print 'stats file %s missing' % (STATS_FILE,)
        exit()
    try:
        with open( STATS_FILE ) as f:
            print f.read(),

        # There is a potential race condition here with Munin.  If the stats
        # file takes too long to be written out or Munin is too quick to invoke
        # this routine, it'll get old data and the new data will get lost.
    except OSError:
        pass

    exit()


def do_collector():
    """
    Run the collector in the foreground (debugging)
    """
    global run
    input_thread = threading.Thread( target = input_thread_entrypoint )
    input_thread.start()
    try:
        mainline_entrypoint()
    except KeyboardInterrupt:
        run = False


def signal_handler( signum, frame ):
    """
    Unix signal handler, required for daemon mode.
    """
    import signal

    if signum == signal.SIGTERM:
        global run
        run = False
        exit()


def do_daemon( stdout = None, stderr = None ):
    """
    Run collector as a Unix daemon.  Must have python-daemon installed.
    """
    import daemon
    import grp
    import pwd
    import signal

    # Ensure files are world-readable by default
    os.umask( 0022 )

    # Get IDs of user and group
    uid = pwd.getpwnam( USER ).pw_uid
    gid = grp.getgrnam( GROUP ).gr_gid

    # Check for existing PID file and, if exists, check whether there is a
    # daemon already running
    if os.path.exists( PID_FILE ):
        pid = open( PID_FILE ).read().strip()
        if pid:
            try:
                pid = int(pid)
            except ValueError:
                logger.error( "Bad PID in %s: %s" % (PID_FILE, pid,) )
            # Check whether in fact actually running
            try:
                os.kill( pid, 0 )
            except OSError:
                pass
            else:
                logger.error( "Attempt to start second instance" )
                exit(1)

    # Make PID and stats files writeable by unprivileged user
    open( PID_FILE, 'w' ).close()
    os.chown( PID_FILE, uid, gid )
    open( STATS_FILE, 'w' ).close()
    os.chown( STATS_FILE, uid, gid )

    # Prepare daemon environment
    context = daemon.DaemonContext(
                detach_process = True,
                umask = 0022,
                uid = uid,
                gid = gid,
                stderr = stderr,
                stdout = stdout,
                signal_map = { signal.SIGTERM: signal_handler, },
                )
    with context:
        # Now in forked daemon, so write out PID file
        pid = os.getpid()
        with open( PID_FILE, 'w' ) as f:
            print >>f, pid

        logger.info( "daemon started (pid = %s)" % (pid,) )
        do_collector()
    exit(0)



logger = logging.getLogger( 'adsb-msg-dist' )
logger.setLevel( logging.INFO )
log_fmt = logging.Formatter( '%(asctime)s %(levelname)s: %(message)s ' )

# Handle Munin
if len(sys.argv) == 2 and sys.argv[1] == 'config':
    munin_config()
elif len(sys.argv) == 1:
    munin_data()

# Collect data
elif len(sys.argv) == 2 and sys.argv[1] == 'fg':
    log_sh = logging.StreamHandler()
    log_sh.setFormatter( log_fmt )
    logger.addHandler( log_sh )
    do_collector()

elif len(sys.argv) == 2 and sys.argv[1] == 'daemon':
    log_fh = logging.handlers.WatchedFileHandler( LOG_FILE )
    log_fh.setFormatter( log_fmt )
    logger.addHandler( log_fh )
    stdout = StreamToLogger( logger, log_fh, logging.INFO )
    stderr = StreamToLogger( logger, log_fh, logging.ERROR )
    do_daemon( stdout, stderr )

else:
    print 'Munin usage    : %s [config]' % (os.path.basename( sys.argv[0] ),)
    print 'Collector usage: %s <fg|daemon>' % (os.path.basename( sys.argv[0] ),)
    exit(1)

# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4
